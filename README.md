# ðŸ•µï¸â€â™‚ï¸ Spyfall Arena

**Spyfall Arena** is a platform where multiple large language models (LLMs) autonomously play the social deduction game **Spyfall**.
It aims to evaluate how different LLMs perform in reasoning, deception, and deduction under dynamic multi-agent interactions.

---

## ðŸŽ¯ Project Highlights

- ðŸ§  Multi-agent environment for reasoning and bluffing
- ðŸŽ® Automated Spyfall gameplay simulation
- ðŸ“Š Model performance and deception benchmarking
- ðŸ“ Fully backend (no frontend)
- ðŸ§© Configurable via YAML file
- ðŸ“„ JSON logs for every match (questions, answers, votes, results)

---

## ðŸš€ Project Phases

### **Phase 1 â€“ Foundational Arena (MVP)**
> Build the core game engine and single-game simulation.

- [x] Define game rules and structure (Spy vs Civilians)
- [x] Role assignment logic
- [x] Turn-based Q/A flow
- [x] Voting and win condition
- [x] YAML config file for setup
- [x] Structured JSON logging
- [ ] Basic evaluation metrics (win rate, suspicion rate)

---

### **Phase 2 â€“ Comparative Arena**
> Expand to large-scale experimentation and evaluation.

- [ ] Tournament automation (multiple games)
- [ ] Configurable model pool and parameters
- [ ] Aggregate metrics and summary reports
- [ ] Optional â€œJudge LLMâ€ for reasoning evaluation

---

### **Phase 3 â€“ Analytical Arena**
> Enable advanced behavioral and cognitive analysis.

- [ ] Capture hidden reasoning traces
- [ ] Compare internal reasoning vs public responses
- [ ] Support different personality prompts
- [ ] Export datasets for linguistic and statistical study

---

## ðŸ§­ Current Status

> âœ… **Phase 1 in progress**
> Building the foundational backend and refining role-based LLM interactions.

---

**Spyfall Arena**
*Exploring how machines reason, bluff, and deduce â€” one game at a time.*
